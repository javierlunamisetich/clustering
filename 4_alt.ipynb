{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                                           \n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import spdiags\n",
    "from scipy.stats import multivariate_normal\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Wikipedia data and extract TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = pd.read_csv('people_wiki.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous assignment, we extract the TF-IDF vector of each document. For your convenience, we extracted the TF-IDF vectors from the dataset. The vectors are packaged in a sparse matrix, where the i-th row gives the TF-IDF vectors for the i-th document. Each column corresponds to a unique word appearing in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    data = loader['data']\n",
    "    indices = loader['indices']\n",
    "    indptr = loader['indptr']\n",
    "    shape = loader['shape']\n",
    "    \n",
    "    return csr_matrix( (data, indices, indptr), shape)\n",
    "\n",
    "\n",
    "tf_idf = load_sparse_csr('4_tf_idf.npz')\n",
    "import json\n",
    "with open('4_map_index_to_word.json', 'r') as f: # Reads the list of most frequent words\n",
    "    map_index_to_word = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous assignment, we will normalize each document's TF-IDF vector to be a unit vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5000x100282 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 881415 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf = normalize(tf_idf)\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EM in high dimensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EM for high-dimensional data requires some special treatment:\n",
    "\n",
    "E step and M step must be vectorized as much as possible, as explicit loops are dreadfully slow in Python.\n",
    "All operations must be cast in terms of sparse matrix operations, to take advantage of computational savings enabled by sparsity of data.\n",
    "Initially, some words may be entirely absent from a cluster, causing the M step to produce zero mean and variance for those words. This means any data point with one of those words will have 0 probability of being assigned to that cluster since the cluster allows for no variability (0 variance) around that count being 0 (0 mean). Since there is a small chance for those words to later appear in the cluster, we instead assign a small positive variance (~1e-10). Doing so also prevents numerical overflow.\n",
    "Due to complexity in implementation, we provide the complete implementation here. You are expected to answer some quiz questions using the results of clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log probability function for diagonal covariance Gaussian.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diag(array):\n",
    "    n = len(array)\n",
    "    return spdiags(array, 0, n, n)\n",
    "\n",
    "def logpdf_diagonal_gaussian(x, mean, cov):\n",
    "    '''\n",
    "    Compute logpdf of a multivariate Gaussian distribution with diagonal covariance at a given point x.\n",
    "    A multivariate Gaussian distribution with a diagonal covariance is equivalent\n",
    "    to a collection of independent Gaussian random variables.\n",
    "\n",
    "    x should be a sparse matrix. The logpdf will be computed for each row of x.\n",
    "    mean and cov should be given as 1D numpy arrays\n",
    "    mean[i] : mean of i-th variable\n",
    "    cov[i] : variance of i-th variable'''\n",
    "\n",
    "    n = x.shape[0]\n",
    "    dim = x.shape[1]\n",
    "    assert(dim == len(mean) and dim == len(cov))\n",
    "\n",
    "    # multiply each i-th column of x by (1/(2*sigma_i)), where sigma_i is sqrt of variance of i-th variable.\n",
    "    scaled_x = x.dot( diag(1./(2*np.sqrt(cov))) )\n",
    "    # multiply each i-th entry of mean by (1/(2*sigma_i))\n",
    "    scaled_mean = mean/(2*np.sqrt(cov))\n",
    "\n",
    "    # sum of pairwise squared Eulidean distances gives SUM[(x_i - mean_i)^2/(2*sigma_i^2)]\n",
    "    return -np.sum(np.log(np.sqrt(2*np.pi*cov))) - pairwise_distances(scaled_x, [scaled_mean], 'euclidean').flatten()**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EM algorithm for sparse data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum_exp(x, axis):\n",
    "    '''Compute the log of a sum of exponentials'''\n",
    "    x_max = np.max(x, axis=axis)\n",
    "    if axis == 1:\n",
    "        return x_max + np.log( np.sum(np.exp(x-x_max[:,np.newaxis]), axis=1) )\n",
    "    else:\n",
    "        return x_max + np.log( np.sum(np.exp(x-x_max), axis=0) )\n",
    "\n",
    "def EM_for_high_dimension(data, means, covs, weights, cov_smoothing=1e-5, maxiter=int(1e3), thresh=1e-4, verbose=False):\n",
    "    # cov_smoothing: specifies the default variance assigned to absent features in a cluster.\n",
    "    #                If we were to assign zero variances to absent features, we would be overconfient,\n",
    "    #                as we hastily conclude that those featurese would NEVER appear in the cluster.\n",
    "    #                We'd like to leave a little bit of possibility for absent features to show up later.\n",
    "    n = data.shape[0]\n",
    "    dim = data.shape[1]\n",
    "    mu = deepcopy(means)\n",
    "    Sigma = deepcopy(covs)\n",
    "    K = len(mu)\n",
    "    weights = np.array(weights)\n",
    "\n",
    "    ll = None\n",
    "    ll_trace = []\n",
    "\n",
    "    for i in range(maxiter):\n",
    "        # E-step: compute responsibilities\n",
    "        logresp = np.zeros((n,K))\n",
    "        for k in xrange(K):\n",
    "            logresp[:,k] = np.log(weights[k]) + logpdf_diagonal_gaussian(data, mu[k], Sigma[k])\n",
    "        ll_new = np.sum(log_sum_exp(logresp, axis=1))\n",
    "        if verbose:\n",
    "            print(ll_new)\n",
    "        logresp -= np.vstack(log_sum_exp(logresp, axis=1))\n",
    "        resp = np.exp(logresp)\n",
    "        counts = np.sum(resp, axis=0)\n",
    "\n",
    "        # M-step: update weights, means, covariances\n",
    "        weights = counts / np.sum(counts)\n",
    "        for k in range(K):\n",
    "            mu[k] = (diag(resp[:,k]).dot(data)).sum(axis=0)/counts[k]\n",
    "            mu[k] = mu[k].A1\n",
    "\n",
    "            Sigma[k] = diag(resp[:,k]).dot( data.multiply(data)-2*data.dot(diag(mu[k])) ).sum(axis=0) \\\n",
    "                       + (mu[k]**2)*counts[k]\n",
    "            Sigma[k] = Sigma[k].A1 / counts[k] + cov_smoothing*np.ones(dim)\n",
    "\n",
    "        # check for convergence in log-likelihood\n",
    "        ll_trace.append(ll_new)\n",
    "        if ll is not None and (ll_new-ll) < thresh and ll_new > -np.inf:\n",
    "            ll = ll_new\n",
    "            break\n",
    "        else:\n",
    "            ll = ll_new\n",
    "\n",
    "    out = {'weights':weights,'means':mu,'covs':Sigma,'loglik':ll_trace,'resp':resp}\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "np.random.seed(5)\n",
    "num_clusters = 25\n",
    "\n",
    "# Use scikit-learn's k-means to simplify workflow\n",
    "\n",
    "kmeans_model = KMeans(n_clusters=num_clusters, n_init=5, max_iter=400, random_state=1, n_jobs=1)\n",
    "kmeans_model.fit(tf_idf)\n",
    "centroids, cluster_assignment = kmeans_model.cluster_centers_, kmeans_model.labels_\n",
    "\n",
    "means = [centroid for centroid in centroids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = tf_idf.shape[0]\n",
    "weights = []\n",
    "for i in xrange(num_clusters):\n",
    "    # Compute the number of data points assigned to cluster i:\n",
    "    num_assigned = sum(cluster_assignment == i) # YOUR CODE HERE\n",
    "    w = float(num_assigned) / num_docs\n",
    "    weights.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "covs = []\n",
    "for i in xrange(num_clusters):\n",
    "    member_rows = tf_idf[cluster_assignment==i]\n",
    "    cov = (member_rows.multiply(member_rows) - 2*member_rows.dot(diag(means[i]))).sum(axis=0).A1 / member_rows.shape[0] \\\n",
    "          + means[i]**2\n",
    "    cov[cov < 1e-8] = 1e-8\n",
    "    covs.append(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3879297479.366981, 4883345753.533131, 4883345753.533131]\n"
     ]
    }
   ],
   "source": [
    "out = EM_for_high_dimension(tf_idf, means, covs, weights, cov_smoothing=1e-10)\n",
    "print out['loglik'] # print history of log-likelihood over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_EM_clusters(tf_idf, means, covs, map_index_to_word):\n",
    "    print('')\n",
    "    print('==========================================================')\n",
    "\n",
    "    num_clusters = len(means)\n",
    "    for c in xrange(num_clusters):\n",
    "        print('Cluster {0:d}: Largest mean parameters in cluster '.format(c))\n",
    "        print('\\n{0: <12}{1: <12}{2: <12}'.format('Word', 'Mean', 'Variance'))\n",
    "        \n",
    "        # The k'th element of sorted_word_ids should be the index of the word \n",
    "        # that has the k'th-largest value in the cluster mean. Hint: Use np.argsort().\n",
    "        sorted_word_ids = np.argsort(means[c])[::-1]  # YOUR CODE HERE\n",
    "\n",
    "        for i in sorted_word_ids[:5]:\n",
    "            print '{0: <12}{1:<10.2e}{2:10.2e}'.format({v:k for k, v in map_index_to_word.items()}[i], means[c][i], covs[c][i])\n",
    "                                                       \n",
    "        print '\\n=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]===='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================\n",
      "Cluster 0: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "poetry      1.51e-01    1.90e-02\n",
      "poems       6.33e-02    6.45e-03\n",
      "poet        5.91e-02    6.36e-03\n",
      "de          4.77e-02    8.72e-03\n",
      "literary    4.68e-02    3.29e-03\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 1: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         1.60e-01    4.59e-03\n",
      "her         1.04e-01    3.20e-03\n",
      "music       1.53e-02    1.04e-03\n",
      "actress     1.52e-02    1.14e-03\n",
      "show        1.27e-02    7.33e-04\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 2: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "football    7.45e-02    4.57e-03\n",
      "club        5.84e-02    2.55e-03\n",
      "league      5.72e-02    2.83e-03\n",
      "season      5.06e-02    2.35e-03\n",
      "played      3.79e-02    9.46e-04\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 3: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "district    5.56e-02    4.00e-03\n",
      "republican  5.47e-02    4.55e-03\n",
      "senate      5.04e-02    5.21e-03\n",
      "democratic  3.72e-02    2.46e-03\n",
      "house       3.65e-02    2.07e-03\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 4: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "novel       5.26e-02    5.50e-03\n",
      "book        4.30e-02    2.38e-03\n",
      "published   3.87e-02    1.51e-03\n",
      "books       3.19e-02    1.57e-03\n",
      "fiction     3.06e-02    3.10e-03\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 5: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "rugby       1.59e-01    1.14e-02\n",
      "against     5.11e-02    1.99e-03\n",
      "wales       4.96e-02    6.96e-03\n",
      "played      4.90e-02    1.42e-03\n",
      "cup         4.81e-02    2.30e-03\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 6: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "tour        5.18e-02    1.08e-02\n",
      "pga         4.42e-02    1.36e-02\n",
      "championship4.31e-02    3.78e-03\n",
      "racing      3.53e-02    4.77e-03\n",
      "won         3.11e-02    6.93e-04\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 7: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "film        1.78e-01    6.05e-03\n",
      "films       6.60e-02    3.33e-03\n",
      "festival    4.46e-02    3.60e-03\n",
      "feature     3.41e-02    1.61e-03\n",
      "directed    2.83e-02    1.95e-03\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 8: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "jazz        1.62e-01    1.65e-02\n",
      "chess       1.09e-01    3.10e-02\n",
      "grandmaster 3.78e-02    8.97e-03\n",
      "music       3.39e-02    1.15e-03\n",
      "pianist     2.69e-02    2.30e-03\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 9: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "engineering 2.15e-02    2.76e-03\n",
      "business    2.08e-02    1.55e-03\n",
      "technology  1.90e-02    1.46e-03\n",
      "company     1.80e-02    9.69e-04\n",
      "chairman    1.69e-02    1.51e-03\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 10: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "art         1.37e-01    6.78e-03\n",
      "museum      6.21e-02    7.84e-03\n",
      "artist      4.17e-02    1.74e-03\n",
      "gallery     3.91e-02    3.67e-03\n",
      "work        3.23e-02    7.92e-04\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 11: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "research    4.17e-02    2.15e-03\n",
      "university  3.85e-02    7.99e-04\n",
      "professor   3.30e-02    1.27e-03\n",
      "science     2.31e-02    2.08e-03\n",
      "studies     2.11e-02    1.76e-03\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 12: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "law         1.13e-01    8.91e-03\n",
      "court       6.40e-02    5.53e-03\n",
      "judge       4.00e-02    4.70e-03\n",
      "justice     3.63e-02    3.37e-03\n",
      "rights      3.47e-02    5.19e-03\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 13: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "coach       9.60e-02    9.97e-03\n",
      "hockey      9.28e-02    2.04e-02\n",
      "soccer      8.30e-02    2.46e-02\n",
      "nhl         6.11e-02    1.22e-02\n",
      "season      4.47e-02    1.93e-03\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 14: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "baseball    1.15e-01    5.53e-03\n",
      "league      1.03e-01    3.64e-03\n",
      "major       5.10e-02    1.16e-03\n",
      "games       4.71e-02    1.94e-03\n",
      "sox         4.57e-02    6.25e-03\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 15: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "basketball  9.90e-02    1.32e-02\n",
      "football    5.98e-02    5.74e-03\n",
      "yards       5.32e-02    1.39e-02\n",
      "nba         5.21e-02    8.68e-03\n",
      "nfl         4.69e-02    7.80e-03\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 16: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "church      1.20e-01    9.69e-03\n",
      "bishop      8.51e-02    1.06e-02\n",
      "archbishop  4.87e-02    7.44e-03\n",
      "diocese     4.70e-02    5.52e-03\n",
      "lds         4.13e-02    9.88e-03\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 17: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "that        1.30e-02    1.97e-04\n",
      "i           1.19e-02    1.58e-03\n",
      "were        1.07e-02    2.83e-04\n",
      "he          1.03e-02    5.77e-05\n",
      "had         1.01e-02    2.24e-04\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 18: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "music       1.12e-01    5.23e-03\n",
      "orchestra   8.40e-02    9.74e-03\n",
      "symphony    5.39e-02    8.05e-03\n",
      "conductor   4.97e-02    7.52e-03\n",
      "opera       4.84e-02    1.30e-02\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 19: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         9.29e-02    8.21e-03\n",
      "championships8.06e-02    6.81e-03\n",
      "miss        7.95e-02    2.83e-02\n",
      "marathon    6.87e-02    2.49e-02\n",
      "metres      5.87e-02    1.08e-02\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 20: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minister    1.28e-01    7.34e-03\n",
      "prime       4.89e-02    4.79e-03\n",
      "government  3.65e-02    2.00e-03\n",
      "party       3.32e-02    1.33e-03\n",
      "cabinet     3.25e-02    3.34e-03\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 21: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "news        5.64e-02    6.42e-03\n",
      "radio       4.04e-02    3.61e-03\n",
      "show        3.08e-02    2.28e-03\n",
      "television  2.40e-02    1.07e-03\n",
      "reporter    2.38e-02    1.88e-03\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 22: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "theatre     5.66e-02    7.14e-03\n",
      "film        4.28e-02    1.66e-03\n",
      "actor       3.92e-02    3.01e-03\n",
      "television  3.67e-02    1.74e-03\n",
      "comedy      3.52e-02    4.57e-03\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 23: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "party       6.80e-02    3.06e-03\n",
      "election    6.29e-02    3.42e-03\n",
      "elected     3.82e-02    1.24e-03\n",
      "liberal     3.56e-02    5.48e-03\n",
      "council     3.18e-02    2.33e-03\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 24: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "album       7.41e-02    4.96e-03\n",
      "band        5.63e-02    4.27e-03\n",
      "music       4.36e-02    2.12e-03\n",
      "released    3.28e-02    1.14e-03\n",
      "records     2.34e-02    1.21e-03\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n"
     ]
    }
   ],
   "source": [
    "visualize_EM_clusters(tf_idf, out['means'], out['covs'], map_index_to_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  7, 12, 56,  2, 98,  4])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eje=np.array([1,7,12,56,2,98,4])\n",
    "eje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 3, 2, 1, 6, 4, 0])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(eje)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
